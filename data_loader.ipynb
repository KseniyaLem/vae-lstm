{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data loader.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTiBw6q_jVMr"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "from matplotlib.pyplot import savefig\n",
        "import os\n",
        "import random\n",
        "\n",
        "class BaseDataGenerator:\n",
        "  def __init__(self, config):\n",
        "    self.config = config\n",
        "\n",
        "  # separate training and val sets\n",
        "  def separate_train_and_val_set(self, n_win):\n",
        "    n_train = int(np.floor((n_win * 0.9)))\n",
        "    n_val = n_win - n_train\n",
        "    idx_train = random.sample(range(n_win), n_train)\n",
        "    idx_val = list(set(idx_train) ^ set(range(n_win)))\n",
        "    return idx_train, idx_val, n_train, n_val\n",
        "\n",
        "\n",
        "class DataGenerator(BaseDataGenerator):\n",
        "  def __init__(self, dataset, config):\n",
        "    super(DataGenerator, self).__init__(config)\n",
        "    # load data here: generate 3 state variables: train_set, val_set and test_set\n",
        "    self.load_NAB_dataset(dataset, self.config['y_scale'])\n",
        "\n",
        "  def load_NAB_dataset(self, dataset, y_scale=6):\n",
        "    data = dataset\n",
        "    # normalise the dataset by training set mean and std\n",
        "    train_m = data['train_m']\n",
        "    train_std = data['train_std']\n",
        "    readings_normalised = (data['readings'] - train_m) / train_std\n",
        "\n",
        "    # plot normalised data\n",
        "    fig, axs = plt.subplots(1, 1, figsize=(18, 4), edgecolor='k')\n",
        "    fig.subplots_adjust(hspace=.4, wspace=.4)\n",
        "    axs.plot(data['t'], readings_normalised)\n",
        "    if data['idx_split'][0] == 0:\n",
        "      axs.plot(data['idx_split'][1] * np.ones(20), np.linspace(-y_scale, y_scale, 20), 'b-')\n",
        "    else:\n",
        "      for i in range(2):\n",
        "        axs.plot(data['idx_split'][i] * np.ones(20), np.linspace(-y_scale, y_scale, 20), 'b-')\n",
        "    # axs.plot(*np.ones(20), np.linspace(-y_scale, y_scale, 20), 'b--')\n",
        "    for j in range(len(data['idx_anomaly'])):\n",
        "      axs.plot(data['idx_anomaly'][j] * np.ones(20), np.linspace(-y_scale, 0.75 * y_scale, 20), 'r--')\n",
        "    axs.grid(True)\n",
        "    axs.set_xlim(0, len(data['t']))\n",
        "    axs.set_ylim(-y_scale, y_scale)\n",
        "    axs.set_xlabel(\"timestamp (every {})\".format(data['t_unit']))\n",
        "    axs.set_ylabel(\"readings\")\n",
        "    axs.set_title(\"{} dataset\\n(normalised by train mean {:.4f} and std {:.4f})\".format(dataset, train_m, train_std))\n",
        "    axs.legend(('data', 'train test set split', 'anomalies'))\n",
        "\n",
        "    # slice training set into rolling windows\n",
        "    n_train_sample = len(data['training'])\n",
        "    n_train_vae = n_train_sample - self.config['l_win'] + 1\n",
        "    rolling_windows = np.zeros((n_train_vae, self.config['l_win']))\n",
        "    for i in range(n_train_sample - self.config['l_win'] + 1):\n",
        "      rolling_windows[i] = data['training'][i:i + self.config['l_win']]\n",
        "\n",
        "    # create VAE training and validation set\n",
        "    idx_train, idx_val, self.n_train_vae, self.n_val_vae = self.separate_train_and_val_set(n_train_vae)\n",
        "    self.train_set_vae = dict(data=np.expand_dims(rolling_windows[idx_train], -1))\n",
        "    self.val_set_vae = dict(data=np.expand_dims(rolling_windows[idx_val], -1))\n",
        "    self.test_set_vae = dict(data=np.expand_dims(rolling_windows[idx_val[:self.config['batch_size']]], -1))\n",
        "\n",
        "    # create LSTM training and validation set\n",
        "    for k in range(self.config['l_win']):\n",
        "      n_not_overlap_wins = (n_train_sample - k) // self.config['l_win']\n",
        "      n_train_lstm = n_not_overlap_wins - self.config['l_seq'] + 1\n",
        "      cur_lstm_seq = np.zeros((n_train_lstm, self.config['l_seq'], self.config['l_win']))\n",
        "      for i in range(n_train_lstm):\n",
        "        cur_seq = np.zeros((self.config['l_seq'], self.config['l_win']))\n",
        "        for j in range(self.config['l_seq']):\n",
        "          # print(k,i,j)\n",
        "          cur_seq[j] = data['training'][k + self.config['l_win'] * (j + i): k + self.config['l_win'] * (j + i + 1)]\n",
        "        cur_lstm_seq[i] = cur_seq\n",
        "      if k == 0:\n",
        "        lstm_seq = cur_lstm_seq\n",
        "      else:\n",
        "        lstm_seq = np.concatenate((lstm_seq, cur_lstm_seq), axis=0)\n",
        "\n",
        "    n_train_lstm = lstm_seq.shape[0]\n",
        "    idx_train, idx_val, self.n_train_lstm, self.n_val_lstm = self.separate_train_and_val_set(n_train_lstm)\n",
        "    self.train_set_lstm = dict(data=np.expand_dims(lstm_seq[idx_train], -1))\n",
        "    self.val_set_lstm = dict(data=np.expand_dims(lstm_seq[idx_val], -1))\n",
        "\n",
        "  def plot_time_series(self, data, time, data_list):\n",
        "    fig, axs = plt.subplots(1, 4, figsize=(18, 2.5), edgecolor='k')\n",
        "    fig.subplots_adjust(hspace=.8, wspace=.4)\n",
        "    axs = axs.ravel()\n",
        "    for i in range(4):\n",
        "      axs[i].plot(time / 60., data[:, i])\n",
        "      axs[i].set_title(data_list[i])\n",
        "      axs[i].set_xlabel('time (h)')\n",
        "      axs[i].set_xlim((np.amin(time) / 60., np.amax(time) / 60.))"
      ],
      "execution_count": 1,
      "outputs": []
    }
  ]
}